0	0
Proceedings	0
of	0
the	0
49th	0
Annual	0
Meeting	0
of	0
the	0
Association	0
for	0
Computational	0
Linguistics,	0
pages	0
93101,	0
0	0
Portland,	0
Oregon,	0
June	0
19-24,	0
2011.	0
c	0
0	0
2011	0
Association	0
for	0
Computational	0
Linguistics	0
0	0
Semi-Supervised	0
SimHash	0
for	0
Efficient	0
Document	0
Similarity	0
Search	0
0	0
Qixia	0
Jiang	0
and	0
Maosong	0
Sun	0
0	0
State	0
Key	0
Laboratory	0
on	0
Intelligent	0
Technology	0
and	0
Systems	0
0	0
Tsinghua	0
National	0
Laboratory	0
for	0
Information	0
Science	0
and	0
Technology	0
0	0
Department	0
of	0
Computer	0
Sci.	0
and	0
Tech.,	0
Tsinghua	0
University,	0
Beijing	0
100084,	0
China	0
0	0
qixia.jiang@gmail.com,	0
sms@tsinghua.edu.cn	0
0	0
Abstract	0
0	0
Searching	0
documents	0
that	0
are	0
similar	0
to	0
a	0
0	0
query	0
document	0
is	0
an	0
important	0
component	0
0	0
in	0
modern	0
information	0
retrieval.	0
Some	0
ex-	0
0	0
isting	0
hashing	0
methods	0
can	0
be	0
used	0
for	0
effi-	0
0	0
cient	0
document	0
similarity	0
search.	0
However,	0
0	0
unsupervised	0
hashing	0
methods	0
cannot	0
incor-	0
0	0
porate	0
prior	0
knowledge	0
for	0
better	0
hashing.	0
0	0
Although	0
some	0
supervised	0
hashing	0
methods	0
0	0
can	0
derive	0
effective	0
hash	0
functions	0
from	0
prior	0
0	0
knowledge,	0
they	0
are	0
either	0
computationally	0
0	0
expensive	0
or	0
poorly	0
discriminative.	0
This	0
pa-	0
0	0
per	0
proposes	0
a	0
novel	0
(semi-)supervised	0
hash-	0
0	0
ing	0
method	0
named	0
Semi-Supervised	0
SimHash	0
0	0
(S	0
3	0
H)	0
for	0
high-dimensional	0
data	0
similarity	0
0	0
search.	0
The	0
basic	0
idea	0
of	0
S	0
3	0
H	0
is	0
to	0
learn	0
the	0
0	0
optimal	0
feature	0
weights	0
from	0
prior	0
knowledge	0
0	0
to	0
relocate	0
the	0
data	0
such	0
that	0
similar	0
data	0
have	0
0	0
similar	0
hash	0
codes.	0
We	0
evaluate	0
our	0
method	0
0	0
with	0
several	0
state-of-the-art	0
methods	0
on	0
two	0
0	0
large	0
datasets.	0
All	0
the	0
results	0
show	0
that	0
our	0
0	0
method	0
gets	0
the	0
best	0
performance.	0
0	0
1	0
Introduction	0
0	0
Document	0
Similarity	0
Search	0
(DSS)	0
is	0
to	0
find	0
sim-	0
0	0
ilar	0
documents	0
to	0
a	0
query	0
doc	0
in	0
a	0
text	0
corpus	0
or	0
0	0
on	0
the	0
web.	0
It	0
is	0
an	0
important	0
component	0
in	0
mod-	0
0	0
ern	0
information	0
retrieval	0
since	0
DSS	0
can	0
improve	0
the	0
0	0
traditional	0
search	0
engines	0
and	0
user	0
experience	0
(Wan	0
0	0
et	0
al.,	0
2008;	0
Dean	0
et	0
al.,	0
1999).	0
Traditional	0
search	0
0	0
engines	0
accept	0
several	0
terms	0
submitted	0
by	0
a	0
user	0
0	0
as	0
a	0
query	0
and	0
return	0
a	0
set	0
of	0
docs	0
that	0
are	0
rele-	0
0	0
vant	0
to	0
the	0
query.	0
However,	0
for	0
those	0
users	0
who	0
0	0
are	0
not	0
search	0
experts,	0
it	0
is	0
always	0
difficult	0
to	0
ac-	0
0	0
curately	0
specify	0
some	0
query	0
terms	0
to	0
express	0
their	0
0	0
search	0
purposes.	0
Unlike	0
short-query	0
based	0
search,	0
0	0
DSS	0
queries	0
by	0
a	0
full	0
(long)	0
document,	0
which	0
allows	0
0	0
users	0
to	0
directly	0
submit	0
a	0
page	0
or	0
a	0
document	0
to	0
the	0
0	0
search	0
engines	0
as	0
the	0
description	0
of	0
their	0
informa-	0
0	0
tion	0
needs.	0
Meanwhile,	0
the	0
explosion	0
of	0
information	0
0	0
has	0
brought	0
great	0
challenges	0
to	0
traditional	0
methods.	0
0	0
For	0
example,	0
Inverted	0
List	0
(IL)	0
which	0
is	0
a	0
primary	0
0	0
key-term	0
access	0
method	0
would	0
return	0
a	0
very	0
large	0
0	0
set	0
of	0
docs	0
for	0
a	0
query	0
document,	0
which	0
leads	0
to	0
the	0
0	0
time-consuming	0
post-processing.	0
Therefore,	0
a	0
new	0
0	0
effective	0
algorithm	0
is	0
required.	0
0	0
Hashing	0
methods	0
can	0
perform	0
highly	0
efficient	0
but	0
0	0
approximate	0
similarity	0
search,	0
and	0
have	0
gained	0
great	0
0	0
success	0
in	0
many	0
applications	0
such	0
as	0
Content-Based	0
0	0
Image	0
Retrieval	0
(CBIR)	0
(Ke	0
et	0
al.,	0
2004;	0
Kulis	0
et	0
0	0
al.,	0
2009b),	0
near-duplicate	0
data	0
detection	0
(Ke	0
et	0
0	0
al.,	0
2004;	0
Manku	0
et	0
al.,	0
2007;	0
Costa	0
et	0
al.,	0
2010),	0
0	0
etc.	0
Hashing	0
methods	0
project	0
high-dimensional	0
ob-	0
0	0
jects	0
to	0
compact	0
binary	0
codes	0
called	0
fingerprints	0
and	0
0	0
make	0
similar	0
fingerprints	0
for	0
similar	0
objects.	0
The	0
0	0
similarity	0
search	0
in	0
the	0
Hamming	0
space	0
1	0
is	0
much	0
0	0
more	0
efficient	0
than	0
in	0
the	0
original	0
attribute	0
space	0
0	0
(Manku	0
et	0
al.,	0
2007).	0
0	0
Recently,	0
several	0
hashing	0
methods	0
have	0
been	0
pro-	0
0	0
posed.	0
Specifically,	0
SimHash	0
(SH)	0
(Charikar	0
M.S.,	0
0	0
2002)	0
uses	0
random	0
projections	0
to	0
hash	0
data.	0
Al-	0
0	0
though	0
it	0
works	0
well	0
with	0
long	0
fingerprints,	0
SH	0
has	0
0	0
poor	0
discrimination	0
power	0
for	0
short	0
fingerprints.	0
A	0
0	0
kernelized	0
variant	0
of	0
SH,	0
called	0
Kernelized	0
Local-	0
0	0
ity	0
Sensitive	0
Hashing	0
(KLSH)	0
(Kulis	0
et	0
al.,	0
2009a),	0
0	0
is	0
proposed	0
to	0
handle	0
non-linearly	0
separable	0
data.	0
0	0
These	0
methods	0
are	0
unsupervised	0
thus	0
cannot	0
incor-	0
0	0
porate	0
prior	0
knowledge	0
for	0
better	0
hashing.	0
Moti-	0
0	0
1	0
Hamming	0
space	0
is	0
a	0
set	0
of	0
binary	0
strings	0
of	0
length	0
L.	0
0	0
93	0
0	0
vated	0
by	0
this,	0
some	0
supervised	0
methods	0
are	0
pro-	0
0	0
posed	0
to	0
derive	0
effective	0
hash	0
functions	0
from	0
prior	0
0	0
knowledge,	0
i.e.,	0
Spectral	0
Hashing	0
(Weiss	0
et	0
al.,	0
0	0
2009)	0
and	0
Semi-Supervised	0
Hashing	0
(SSH)	0
(Wang	0
0	0
et	0
al.,	0
2010a).	0
Regardless	0
of	0
different	0
objectives,	0
0	0
both	0
methods	0
derive	0
hash	0
functions	0
via	0
Principle	0
0	0
Component	0
Analysis	0
(PCA)	0
(Jolliffe,	0
1986).	0
How-	0
0	0
ever,	0
PCA	0
is	0
computationally	0
expensive,	0
which	0
lim-	0
0	0
its	0
their	0
usage	0
for	0
high-dimensional	0
data.	0
0	0
This	0
paper	0
proposes	0
a	0
novel	0
(semi-)supervised	0
0	0
hashing	0
method,	0
Semi-Supervised	0
SimHash	0
(S	0
3	0
H),	0
0	0
for	0
high-dimensional	0
data	0
similarity	0
search.	0
Un-	0
0	0
like	0
SSH	0
that	0
tries	0
to	0
find	0
a	0
sequence	0
of	0
hash	0
func-	0
0	0
tions,	0
S	0
3	0
H	0
fixes	0
the	0
random	0
projection	0
directions	0
0	0
and	0
seeks	0
the	0
optimal	0
feature	0
weights	0
from	0
prior	0
0	0
knowledge	0
to	0
relocate	0
the	0
objects	0
such	0
that	0
simi-	0
0	0
lar	0
objects	0
have	0
similar	0
fingerprints.	0
This	0
is	0
im-	0
0	0
plemented	0
by	0
maximizing	0
the	0
empirical	0
accuracy	0
0	0
on	0
the	0
prior	0
knowledge	0
(labeled	0
data)	0
and	0
the	0
en-	0
0	0
tropy	0
of	0
hash	0
functions	0
(estimated	0
over	0
labeled	0
and	0
0	0
unlabeled	0
data).	0
The	0
proposed	0
method	0
avoids	0
us-	0
0	0
ing	0
PCA	0
which	0
is	0
computationally	0
expensive	0
espe-	0
0	0
cially	0
for	0
high-dimensional	0
data,	0
and	0
leads	0
to	0
an	0
0	0
efficient	0
Quasi-Newton	0
based	0
solution.	0
To	0
evalu-	0
0	0
ate	0
our	0
method,	0
we	0
compare	0
with	0
several	0
state-of-	0
0	0
the-art	0
hashing	0
methods	0
on	0
two	0
large	0
datasets,	0
i.e.,	0
0	0
20	0
Newsgroups	0
(20K	0
points)	0
and	0
Open	0
Directory	0
0	0
Project	0
(ODP)	0
(2.4	0
million	0
points).	0
All	0
experiments	0
0	0
show	0
that	0
S	0
3	0
H	0
gets	0
the	0
best	0
search	0
performance.	0
0	0
This	0
paper	0
is	0
organized	0
as	0
follows:	0
Section	0
2	0
0	0
briefly	0
introduces	0
the	0
background	0
and	0
some	0
related	0
0	0
works.	0
In	0
Section	0
3,	0
we	0
describe	0
our	0
proposed	0
Semi-	0
0	0
Supervised	0
SimHash	0
(S	0
3	0
H).	0
Section	0
4	0
provides	0
ex-	0
0	0
perimental	0
validation	0
on	0
two	0
datasets.	0
The	0
conclu-	0
0	0
sions	0
are	0
given	0
in	0
Section	0
5.	0
0	0
2	0
Background	0
and	0
Related	0
Works	0
0	0
Suppose	0
we	0
are	0
given	0
a	0
set	0
of	0
N	0
documents,	0
X	0
=	0
0	0
{x	0
i	0
|	0
x	0
i	0
R	0
M	0
}	0
N	0
0	0
i=1	0
.	0
For	0
a	0
given	0
query	0
doc	0
q,	0
DSS	0
0	0
tries	0
to	0
find	0
its	0
nearest	0
neighbors	0
in	0
X	0
or	0
a	0
subset	0
0	0
X	0
X	0
in	0
which	0
distance	0
from	0
the	0
documents	0
to	0
0	0
the	0
query	0
doc	0
q	0
is	0
less	0
than	0
a	0
give	0
threshold.	0
How-	0
0	0
ever,	0
such	0
two	0
tasks	0
are	0
computationally	0
infeasible	0
0	0
for	0
large-scale	0
data.	0
Thus,	0
it	0
turns	0
to	0
the	0
approxi-	0
0	0
mate	0
similarity	0
search	0
problem	0
(Indyk	0
et	0
al.,	0
1998).	0
0	0
In	0
this	0
section,	0
we	0
briefly	0
review	0
some	0
related	0
ap-	0
0	0
proximate	0
similarity	0
search	0
methods.	0
0	0
2.1	0
SimHash	0
0	0
SimHash	0
(SH)	0
is	0
first	0
proposed	0
by	0
Charikar	0
0	0
(Charikar	0
M.S.,	0
2002).	0
SH	0
uses	0
random	0
projections	0
0	0
as	0
hash	0
functions,	0
i.e.,	0
0	0
h(x)	0
=	0
sign(w	0
T	0
x)	0
=	0
0	0
{	0
0	0
+1,	0
if	0
w	0
T	0
x	0
0	0
0	0
1,	0
otherwise	0
0	0
(1)	0
0	0
where	0
w	0
R	0
M	0
is	0
a	0
vector	0
randomly	0
generated.	0
SH	0
0	0
specifies	0
the	0
distribution	0
on	0
a	0
family	0
of	0
hash	0
func-	0
0	0
tions	0
H	0
=	0
{h}	0
such	0
that	0
for	0
two	0
objects	0
x	0
i	0
and	0
x	0
j	0
,	0
0	0
Pr	0
0	0
hH	0
0	0
{h(x	0
i	0
)	0
=	0
h(x	0
j	0
)}	0
=	0
1	0
0	0
(x	0
i	0
,	0
x	0
j	0
)	0
0	0
0	0
(2)	0
0	0
where	0
(x	0
i	0
,	0
x	0
j	0
)	0
is	0
the	0
angle	0
between	0
x	0
i	0
and	0
x	0
j	0
.	0
Ob-	0
0	0
viously,	0
SH	0
is	0
an	0
unsupervised	0
hashing	0
method.	0
0	0
2.2	0
Kernelized	0
Locality	0
Sensitive	0
Hashing	0
0	0
A	0
kernelized	0
variant	0
of	0
SH,	0
named	0
Kernelized	0
0	0
Locality	0
Sensitive	0
Hashing	0
(KLSH)	0
(Kulis	0
et	0
al.,	0
0	0
2009a),	0
is	0
proposed	0
for	0
non-linearly	0
separable	0
data.	0
0	0
KLSH	0
approximates	0
the	0
underling	0
Gaussian	0
distri-	0
0	0
bution	0
in	0
the	0
implicit	0
embedding	0
space	0
of	0
data	0
based	0
0	0
on	0
central	0
limit	0
theory.	0
To	0
calculate	0
the	0
value	0
of	0
0	0
hashing	0
fuction	0
h(),	0
KLSH	0
projects	0
points	0
onto	0
the	0
0	0
eigenvectors	0
of	0
the	0
kernel	0
matrix.	0
In	0
short,	0
the	0
com-	0
0	0
plete	0
procedure	0
of	0
KLSH	0
can	0
be	0
summarized	0
as	0
fol-	0
0	0
lows:	0
1)	0
randomly	0
select	0
P	0
(a	0
small	0
value)	0
points	0
0	0
from	0
X	0
and	0
form	0
the	0
kernel	0
matrix,	0
2)	0
for	0
each	0
hash	0
0	0
function	0
h((x)),	0
calculate	0
its	0
weight	0
R	0
P	0
just	0
0	0
as	0
Kernel	0
PCA	0
(Sch	0
olkopf	0
et	0
al.,	0
1997),	0
and	0
3)	0
the	0
0	0
hash	0
function	0
is	0
defined	0
as:	0
0	0
h((x))	0
=	0
sign(	0
0	0
P	0
0	0
0	0
i=1	0
0	0
i	0
(x,	0
x	0
i	0
))	0
0	0
(3)	0
0	0
where	0
(,	0
)	0
can	0
be	0
any	0
kernel	0
function.	0
0	0
KLSH	0
can	0
improve	0
hashing	0
results	0
via	0
the	0
kernel	0
0	0
trick.	0
However,	0
KLSH	0
is	0
unsupervised,	0
thus	0
design-	0
0	0
ing	0
a	0
data-specific	0
kernel	0
remains	0
a	0
big	0
challenge.	0
0	0
2.3	0
Semi-Supervised	0
Hashing	0
0	0
Semi-Supervised	0
Hashing	0
(SSH)	0
(Wang	0
et	0
al.,	0
0	0
2010a)	0
is	0
recently	0
proposed	0
to	0
incorporate	0
prior	0
0	0
knowledge	0
for	0
better	0
hashing.	0
Besides	0
X	0
,	0
prior	0
0	0
knowledge	0
in	0
the	0
form	0
of	0
similar	0
and	0
dissimilar	0
0	0
object-pairs	0
is	0
also	0
required	0
in	0
SSH.	0
SSH	0
tries	0
to	0
0	0
find	0
L	0
optimal	0
hash	0
functions	0
which	0
have	0
maximum	0
0	0
94	0
0	0
empirical	0
accuracy	0
on	0
prior	0
knowledge	0
and	0
maxi-	0
0	0
mum	0
entropy	0
by	0
finding	0
the	0
top	0
L	0
eigenvectors	0
of	0
0	0
an	0
extended	0
covariance	0
matrix	0
2	0
via	0
PCA	0
or	0
SVD.	0
0	0
However,	0
despite	0
of	0
the	0
potential	0
problems	0
of	0
nu-	0
0	0
merical	0
stability,	0
SVD	0
requires	0
massive	0
computa-	0
0	0
tional	0
space	0
and	0
O(M	0
3	0
)	0
computational	0
time	0
where	0
0	0
M	0
is	0
feature	0
dimension,	0
which	0
limits	0
its	0
usage	0
for	0
0	0
high-dimensional	0
data	0
(Trefethen	0
et	0
al.,	0
1997).	0
Fur-	0
0	0
thermore,	0
the	0
variance	0
of	0
directions	0
obtained	0
by	0
0	0
PCA	0
decreases	0
with	0
the	0
decrease	0
of	0
the	0
rank	0
(Jol-	0
0	0
liffe,	0
1986).	0
Thus,	0
lower	0
hash	0
functions	0
tend	0
to	0
have	0
0	0
smaller	0
entropy	0
and	0
larger	0
empirical	0
errors.	0
0	0
2.4	0
Others	0
0	0
Some	0
other	0
related	0
works	0
should	0
be	0
mentioned.	0
A	0
0	0
notable	0
method	0
is	0
Locality	0
Sensitive	0
Hashing	0
(LSH)	0
0	0
(Indyk	0
et	0
al.,	0
1998).	0
LSH	0
performs	0
a	0
random	0
0	0
linear	0
projection	0
to	0
map	0
similar	0
objects	0
to	0
similar	0
0	0
hash	0
codes.	0
However,	0
LSH	0
suffers	0
from	0
the	0
effi-	0
0	0
ciency	0
problem	0
that	0
it	0
tends	0
to	0
generate	0
long	0
codes	0
0	0
(Salakhutdinov	0
et	0
al.,	0
2007).	0
LAMP	0
(Mu	0
et	0
al.,	0
0	0
2009)	0
considers	0
each	0
hash	0
function	0
as	0
a	0
binary	0
par-	0
0	0
tition	0
problem	0
as	0
in	0
SVMs	0
(Burges,	0
1998).	0
Spec-	0
0	0
tral	0
Hashing	0
(Weiss	0
et	0
al.,	0
2009)	0
maintains	0
similar-	0
0	0
ity	0
between	0
objects	0
in	0
the	0
reduced	0
Hamming	0
space	0
0	0
by	0
minimizing	0
the	0
averaged	0
Hamming	0
distance	0
3	0
be-	0
0	0
tween	0
similar	0
neighbors	0
in	0
the	0
original	0
Euclidean	0
0	0
space.	0
However,	0
spectral	0
hashing	0
takes	0
the	0
assump-	0
0	0
tion	0
that	0
data	0
should	0
be	0
distributed	0
uniformly,	0
which	0
0	0
is	0
always	0
violated	0
in	0
real-world	0
applications.	0
0	0
3	0
Semi-Supervised	0
SimHash	0
0	0
In	0
this	0
section,	0
we	0
present	0
our	0
hashing	0
method,	0
0	0
named	0
Semi-Supervised	0
SimHash	0
(S	0
3	0
H).	0
Let	0
X	0
L	0
=	0
0	0
{(x	0
1	0
,	0
c	0
1	0
)	0
.	0
.	0
.	0
(x	0
u	0
,	0
c	0
u	0
)}	0
be	0
the	0
labeled	0
data,	0
c	0
0	0
{1	0
.	0
.	0
.	0
C},	0
x	0
R	0
M	0
,	0
and	0
X	0
U	0
=	0
{x	0
u+1	0
.	0
.	0
.	0
x	0
N	0
}	0
the	0
0	0
unlabeled	0
data.	0
Let	0
X	0
=	0
X	0
L	0
X	0
U	0
.	0
Given	0
the	0
0	0
labeled	0
data	0
X	0
L	0
,	0
we	0
construct	0
two	0
sets,	0
attraction	0
0	0
set	0
a	0
and	0
repulsion	0
set	0
r	0
.	0
Specifically,	0
any	0
pair	0
0	0
(x	0
i	0
,	0
x	0
j	0
)	0
a	0
,	0
i,	0
j	0
u,	0
denotes	0
that	0
x	0
i	0
and	0
x	0
j	0
0	0
are	0
in	0
the	0
same	0
class,	0
i.e.,	0
c	0
i	0
=	0
c	0
j	0
,	0
while	0
any	0
pair	0
0	0
(x	0
i	0
,	0
x	0
j	0
)	0
r	0
,	0
i,	0
j	0
u,	0
denotes	0
that	0
c	0
i	0
=	0
c	0
j	0
.	0
Unlike	0
0	0
2	0
The	0
extended	0
covariance	0
matrix	0
is	0
composed	0
of	0
two	0
com-	0
0	0
ponents,	0
one	0
is	0
an	0
unsupervised	0
covariance	0
term	0
and	0
another	0
is	0
0	0
a	0
constraint	0
matrix	0
involving	0
labeled	0
information.	0
0	0
3	0
Hamming	0
distance	0
is	0
defined	0
as	0
the	0
number	0
of	0
bits	0
that	0
are	0
0	0
different	0
between	0
two	0
binary	0
strings.	0
0	0
previews	0
works	0
that	0
attempt	0
to	0
find	0
L	0
optimal	0
hyper-	0
0	0
planes,	0
the	0
basic	0
idea	0
of	0
S	0
3	0
H	0
is	0
to	0
fix	0
L	0
random	0
hy-	0
0	0
perplanes	0
and	0
to	0
find	0
an	0
optimal	0
feature-weight	0
vec-	0
0	0
tor	0
to	0
relocate	0
the	0
objects	0
such	0
that	0
similar	0
objects	0
0	0
have	0
similar	0
codes.	0
0	0
3.1	0
Data	0
Representation	0
0	0
Since	0
L	0
random	0
hyperplanes	0
are	0
fixed,	0
we	0
can	0
rep-	0
0	0
resent	0
a	0
object	0
x	0
X	0
as	0
its	0
relative	0
position	0
to	0
these	0
0	0
random	0
hyperplanes,	0
i.e.,	0
0	0
D	0
=	0
V	0
0	0
(4)	0
0	0
where	0
the	0
element	0
V	0
ml	0
{+1,	0
1,	0
0}	0
of	0
V	0
indi-	0
0	0
cates	0
that	0
the	0
object	0
x	0
is	0
above,	0
below	0
or	0
just	0
in	0
the	0
0	0
l-th	0
hyperplane	0
with	0
respect	0
to	0
the	0
m-th	0
feature,	0
and	0
0	0
=	0
diag(|x	0
1	0
|,	0
|x	0
2	0
|,	0
.	0
.	0
.	0
,	0
|x	0
M	0
|)	0
is	0
a	0
diagonal	0
matrix	0
0	0
which,	0
to	0
some	0
extent,	0
reflects	0
the	0
distance	0
from	0
x	0
0	0
to	0
these	0
hyperplanes.	0
0	0
3.2	0
Formulation	0
0	0
Hashing	0
maps	0
the	0
data	0
set	0
X	0
to	0
an	0
L-dimensional	0
0	0
Hamming	0
space	0
for	0
compact	0
representations.	0
If	0
we	0
0	0
represent	0
each	0
object	0
as	0
Equation	0
(4),	0
the	0
l-th	0
hash	0
0	0
function	0
is	0
then	0
defined	0
as:	0
0	0
h	0
l	0
(x)	0
=	0
l	0
(D)	0
=	0
sign(w	0
T	0
d	0
l	0
)	0
0	0
(5)	0
0	0
where	0
w	0
R	0
M	0
is	0
the	0
feature	0
weight	0
to	0
be	0
deter-	0
0	0
mined	0
and	0
d	0
l	0
is	0
the	0
l-th	0
column	0
of	0
the	0
matrix	0
D.	0
0	0
Intuitively,	0
the	0
contribution	0
of	0
a	0
specific	0
feature	0
0	0
to	0
different	0
classes	0
is	0
different.	0
Therefore,	0
we	0
hope	0
0	0
to	0
incorporate	0
this	0
side	0
information	0
in	0
S	0
3	0
H	0
for	0
better	0
0	0
hashing.	0
Inspired	0
by	0
(Madani	0
et	0
al.,	0
2009),	0
we	0
can	0
0	0
measure	0
this	0
contribution	0
over	0
X	0
L	0
as	0
in	0
Algorithm	0
1.	0
0	0
Clearly,	0
if	0
objects	0
are	0
represented	0
as	0
the	0
occurrence	0
0	0
numbers	0
of	0
features,	0
the	0
output	0
of	0
Algorithm	0
1	0
is	0
0	0
just	0
the	0
conditional	0
probability	0
Pr(class|feature).	0
0	0
Finally,	0
each	0
object	0
(x,	0
c)	0
X	0
L	0
can	0
be	0
represented	0
0	0
as	0
an	0
M	0
L	0
matrix	0
G:	0
0	0
G	0
=	0
diag(	0
1,c	0
,	0
2,c	0
,	0
.	0
.	0
.	0
,	0
M,c	0
)	0
D	0
0	0
(6)	0
0	0
Note	0
that,	0
one	0
pair	0
(x	0
i	0
,	0
x	0
j	0
)	0
in	0
a	0
or	0
r	0
corresponds	0
0	0
to	0
(G	0
i	0
,	0
G	0
j	0
)	0
while	0
(D	0
i	0
,	0
D	0
j	0
)	0
if	0
we	0
ignore	0
features	0
0	0
contribution	0
to	0
different	0
classes.	0
0	0
Furthermore,	0
we	0
also	0
hope	0
to	0
maximize	0
the	0
em-	0
0	0
pirical	0
accuracy	0
on	0
the	0
labeled	0
data	0
a	0
and	0
r	0
and	0
0	0
95	0
0	0
Algorithm	0
1:	0
Feature	0
Contribution	0
Calculation	0
0	0
for	0
each	0
(x,	0
c)	0
XL	0
do	0
0	0
for	0
each	0
f	0
x	0
do	0
0	0
f	0
f	0
+	0
x	0
f	0
;	0
0	0
f,c	0
f,c	0
+	0
x	0
f	0
;	0
0	0
end	0
0	0
end	0
0	0
for	0
each	0
feature	0
f	0
and	0
class	0
c	0
do	0
0	0
f,c	0
0	0
f,c	0
0	0
f	0
0	0
;	0
0	0
end	0
0	0
maximize	0
the	0
entropy	0
of	0
hash	0
functions.	0
So,	0
we	0
de-	0
0	0
fine	0
the	0
following	0
objective	0
for	0
()s:	0
0	0
J(w)	0
=	0
0	0
1	0
0	0
N	0
p	0
0	0
L	0
0	0
0	0
l=1	0
0	0
{	0
0	0
(xi,xj	0
)a	0
0	0
l	0
(x	0
i	0
)	0
l	0
(x	0
j	0
)	0
0	0
0	0
0	0
(xi,xj	0
)r	0
0	0
l	0
(x	0
i	0
)	0
l	0
(x	0
j	0
)	0
0	0
}	0
0	0
+	0
1	0
0	0
L	0
0	0
0	0
l=1	0
0	0
H(	0
l	0
)	0
0	0
(7)	0
0	0
where	0
N	0
p	0
=	0
|	0
a	0
|	0
+	0
|	0
r	0
|	0
is	0
the	0
number	0
of	0
attrac-	0
0	0
tion	0
and	0
repulsion	0
pairs	0
and	0
1	0
is	0
a	0
tradeoff	0
between	0
0	0
two	0
terms.	0
Wang	0
et	0
al.	0
have	0
proven	0
that	0
hash	0
func-	0
0	0
tions	0
with	0
maximum	0
entropy	0
must	0
maximize	0
the	0
0	0
variance	0
of	0
the	0
hash	0
values,	0
and	0
vice-versa	0
(Wang	0
0	0
et	0
al.,	0
2010b).	0
Thus,	0
H(())	0
can	0
be	0
estimated	0
over	0
0	0
the	0
labeled	0
and	0
unlabeled	0
data,	0
X	0
L	0
and	0
X	0
U	0
.	0
0	0
Unfortunately,	0
direct	0
solution	0
for	0
above	0
problem	0
0	0
is	0
non-trivial	0
since	0
Equation	0
(7)	0
is	0
not	0
differentiable.	0
0	0
Thus,	0
we	0
relax	0
the	0
objective	0
and	0
add	0
an	0
additional	0
0	0
regularization	0
term	0
which	0
could	0
effectively	0
avoid	0
0	0
overfitting.	0
Finally,	0
we	0
obtain	0
the	0
total	0
objective:	0
0	0
L(w)	0
=	0
0	0
1	0
0	0
N	0
p	0
0	0
L	0
0	0
0	0
l=1	0
0	0
{	0
0	0
0	0
(G	0
i	0
,G	0
j	0
)a	0
0	0
(w	0
T	0
g	0
i,l	0
)(w	0
T	0
g	0
j,l	0
)	0
0	0
0	0
0	0
(G	0
i	0
,G	0
j	0
)r	0
0	0
(w	0
T	0
g	0
i,l	0
)(w	0
T	0
g	0
j,l	0
)}	0
0	0
+	0
0	0
1	0
0	0
2N	0
0	0
L	0
0	0
0	0
l=1	0
0	0
{	0
0	0
u	0
0	0
0	0
i=1	0
0	0
2	0
(w	0
T	0
g	0
i,l	0
)	0
+	0
0	0
N	0
0	0
0	0
i=u+1	0
0	0
2	0
(w	0
T	0
d	0
i,l	0
)}	0
0	0
0	0
2	0
0	0
2	0
0	0
w	0
2	0
0	0
2	0
0	0
(8)	0
0	0
where	0
g	0
i,l	0
and	0
d	0
i,l	0
denote	0
the	0
l-th	0
column	0
of	0
G	0
i	0
and	0
0	0
D	0
i	0
respectively,	0
and	0
(t)	0
is	0
a	0
piece-wise	0
linear	0
func-	0
0	0
tion	0
defined	0
as:	0
0	0
(t)	0
=	0
0	0
0	0
0	0
0	0
T	0
g	0
0	0
t	0
>	0
T	0
g	0
0	0
t	0
0	0
T	0
g	0
t	0
T	0
g	0
0	0
T	0
g	0
t	0
<	0
T	0
g	0
0	0
(9)	0
0	0
This	0
relaxation	0
has	0
a	0
good	0
intuitive	0
explanation.	0
0	0
That	0
is,	0
similar	0
objects	0
are	0
desired	0
to	0
not	0
only	0
have	0
0	0
the	0
similar	0
fingerprints	0
but	0
also	0
have	0
sufficient	0
large	0
0	0
projection	0
magnitudes,	0
while	0
dissimilar	0
objects	0
are	0
0	0
desired	0
to	0
not	0
only	0
differ	0
in	0
their	0
fingerprints	0
but	0
also	0
0	0
have	0
large	0
projection	0
margin.	0
However,	0
we	0
do	0
not	0
0	0
hope	0
that	0
a	0
small	0
fraction	0
of	0
object-pairs	0
with	0
very	0
0	0
large	0
projection	0
magnitude	0
or	0
margin	0
dominate	0
the	0
0	0
complete	0
model.	0
Thus,	0
a	0
piece-wise	0
linear	0
function	0
0	0
()	0
is	0
applied	0
in	0
S	0
3	0
H.	0
0	0
As	0
a	0
result,	0
Equation	0
(8)	0
is	0
a	0
simply	0
uncon-	0
0	0
strained	0
optimization	0
problem,	0
which	0
can	0
be	0
ef-	0
0	0
ficiently	0
solved	0
by	0
a	0
notable	0
Quasi-Newton	0
algo-	0
0	0
rithm,	0
i.e.,	0
L-BFGS	0
(Liu	0
et	0
al.,	0
1989).	0
For	0
descrip-	0
0	0
tion	0
simplicity,	0
only	0
attraction	0
set	0
a	0
is	0
considered	0
0	0
and	0
the	0
extension	0
to	0
repulsion	0
set	0
r	0
is	0
straightfor-	0
0	0
ward.	0
Thus,	0
the	0
gradient	0
of	0
L(w)	0
is	0
as	0
follows:	0
0	0
L(w)	0
0	0
w	0
0	0
=	0
0	0
1	0
0	0
N	0
p	0
0	0
L	0
0	0
0	0
l=1	0
0	0
{	0
0	0
0	0
(G	0
i	0
,	0
G	0
j	0
)	0
a,	0
0	0
|w	0
T	0
g	0
i,l	0
|	0
Tg	0
0	0
(w	0
T	0
g	0
j,l	0
)	0
g	0
i,l	0
0	0
+	0
0	0
0	0
(G	0
i	0
,	0
G	0
j	0
)	0
a,	0
0	0
|w	0
T	0
g	0
j,l	0
|	0
Tg	0
0	0
(w	0
T	0
g	0
i,l	0
)	0
g	0
j,l	0
0	0
}	0
0	0
(10)	0
0	0
+	0
0	0
1	0
0	0
N	0
0	0
L	0
0	0
0	0
l=1	0
0	0
{	0
0	0
u	0
0	0
0	0
i	0
=	0
1,	0
0	0
|w	0
T	0
g	0
i,l	0
|	0
Tg	0
0	0
(w	0
T	0
g	0
i,l	0
)	0
g	0
i,l	0
0	0
+	0
0	0
N	0
0	0
0	0
i	0
=	0
u	0
+	0
1,	0
0	0
|w	0
T	0
d	0
i,l	0
|	0
Tg	0
0	0
(w	0
T	0
d	0
i,l	0
)	0
d	0
i,l	0
0	0
}	0
0	0
2	0
w	0
0	0
Note	0
that	0
(t)/t	0
=	0
0	0
when	0
|t|	0
>	0
T	0
g	0
.	0
0	0
3.3	0
Fingerprint	0
Generation	0
0	0
When	0
we	0
get	0
the	0
optimal	0
weight	0
w	0
,	0
we	0
generate	0
0	0
fingerprints	0
for	0
given	0
objects	0
through	0
Equation	0
(5).	0
0	0
Then,	0
it	0
tunes	0
to	0
the	0
problem	0
how	0
to	0
efficiently	0
ob-	0
0	0
tain	0
the	0
representation	0
as	0
in	0
Figure	0
4	0
for	0
a	0
object.	0
0	0
After	0
analysis,	0
we	0
find:	0
1)	0
hyperplanes	0
are	0
randomly	0
0	0
generated	0
and	0
we	0
only	0
need	0
to	0
determine	0
which	0
0	0
sides	0
of	0
these	0
hyperplanes	0
the	0
given	0
object	0
lies	0
on,	0
0	0
and	0
2)	0
in	0
real-world	0
applications,	0
objects	0
such	0
as	0
0	0
docs	0
are	0
always	0
very	0
sparse.	0
Thus,	0
we	0
can	0
avoid	0
0	0
heavy	0
computational	0
demands	0
and	0
efficiently	0
gener-	0
0	0
ate	0
fingerprints	0
for	0
objects.	0
0	0
In	0
practice,	0
given	0
an	0
object	0
x,	0
the	0
procedure	0
of	0
0	0
generating	0
an	0
L-bit	0
fingerprint	0
is	0
as	0
follows:	0
it	0
main-	0
0	0
tains	0
an	0
L-dimensional	0
vector	0
initialized	0
to	0
zero.	0
0	0
Each	0
feature	0
f	0
x	0
is	0
firstly	0
mapped	0
to	0
an	0
L-bit	0
0	0
hash	0
value	0
by	0
Jenkins	0
Hashing	0
Function	0
4	0
.	0
Then,	0
0	0
4	0
http://www.burtleburtle.net/bob/hash/doobs.html	1
0	0
96	0
0	0
Algorithm	0
2:	0
Fast	0
Fingerprint	0
Generation	0
0	0
INPUT:	0
x	0
and	0
w	0
;	0
0	0
initialize	0
0,	0
0,	0
,	0
R	0
L	0
;	0
0	0
for	0
each	0
f	0
x	0
do	0
0	0
randomly	0
project	0
f	0
to	0
h	0
f	0
{1,	0
+1}	0
L	0
;	0
0	0
+	0
x	0
f	0
w	0
0	0
f	0
h	0
f	0
;	0
0	0
end	0
0	0
for	0
l	0
=	0
1	0
to	0
L	0
do	0
0	0
if	0
l	0
>	0
0	0
then	0
0	0
l	0
1;	0
0	0
end	0
0	0
end	0
0	0
RETURN	0
;	0
0	0
these	0
L	0
bits	0
increment	0
or	0
decrement	0
the	0
L	0
compo-	0
0	0
nents	0
of	0
the	0
vector	0
by	0
the	0
value	0
x	0
f	0
w	0
0	0
f	0
.	0
After	0
all	0
0	0
features	0
processed,	0
the	0
signs	0
of	0
components	0
deter-	0
0	0
mine	0
the	0
corresponding	0
bits	0
of	0
the	0
final	0
fingerprint.	0
0	0
The	0
complete	0
algorithm	0
is	0
presented	0
in	0
Algorithm	0
2.	0
0	0
3.4	0
Algorithmic	0
Analysis	0
0	0
This	0
section	0
briefly	0
analyzes	0
the	0
relation	0
between	0
0	0
S	0
3	0
H	0
and	0
some	0
existing	0
methods.	0
For	0
analysis	0
sim-	0
0	0
plicity,	0
we	0
assume	0
(t)	0
=	0
t	0
and	0
ignore	0
the	0
regular-	0
0	0
ization	0
terms.	0
So,	0
Equation	0
(8)	0
can	0
be	0
rewritten	0
as	0
0	0
follows:	0
0	0
J(w)	0
S	0
3	0
H	0
=	0
0	0
1	0
0	0
2	0
0	0
w	0
T	0
[	0
0	0
L	0
0	0
0	0
l=1	0
0	0
l	0
(	0
+	0
)	0
T	0
0	0
l	0
]w	0
0	0
(11)	0
0	0
where	0
+	0
0	0
ij	0
equals	0
to	0
1	0
when	0
(x	0
i	0
,	0
x	0
j	0
)	0
a	0
otherwise	0
0	0
0,	0
0	0
ij	0
equals	0
to	0
1	0
when	0
(x	0
i	0
,	0
x	0
j	0
)	0
r	0
otherwise	0
0	0
0,	0
and	0
l	0
=	0
[g	0
1,l	0
.	0
.	0
.	0
g	0
u,l	0
,	0
d	0
u+1,l	0
.	0
.	0
.	0
d	0
N,l	0
].	0
We	0
de-	0
0	0
note	0
0	0
0	0
l	0
l	0
+	0
T	0
0	0
l	0
and	0
0	0
0	0
l	0
l	0
T	0
0	0
l	0
as	0
S	0
+	0
and	0
S	0
0	0
respectively.	0
Therefore,	0
maximizing	0
above	0
function	0
0	0
is	0
equivalent	0
to	0
maximizing	0
the	0
following:	0
0	0
0	0
J(w)	0
S	0
3	0
H	0
=	0
0	0
|w	0
T	0
S	0
+	0
w|	0
0	0
|w	0
T	0
S	0
w|	0
0	0
(12)	0
0	0
Clearly,	0
Equation	0
(12)	0
is	0
analogous	0
to	0
Linear	0
Dis-	0
0	0
criminant	0
Analysis	0
(LDA)	0
(Duda	0
et	0
al.,	0
2000)	0
ex-	0
0	0
cept	0
for	0
the	0
difference:	0
1)	0
measurement.	0
S	0
3	0
H	0
uses	0
0	0
similarity	0
while	0
LDA	0
uses	0
distance.	0
As	0
a	0
result,	0
the	0
0	0
objective	0
function	0
of	0
S	0
3	0
H	0
is	0
just	0
the	0
reciprocal	0
of	0
0	0
LDAs.	0
2)	0
embedding	0
space.	0
LDA	0
seeks	0
the	0
best	0
0	0
separative	0
direction	0
in	0
the	0
original	0
attribute	0
space.	0
In	0
0	0
contrast,	0
S	0
3	0
H	0
firstly	0
maps	0
data	0
from	0
R	0
M	0
to	0
R	0
M	0
L	0
0	0
through	0
the	0
following	0
projection	0
function	0
0	0
(x)	0
=	0
x	0
[diag(sign(r	0
1	0
)),	0
.	0
.	0
.	0
,	0
diag(sign(r	0
L	0
))]	0
(13)	0
0	0
where	0
r	0
l	0
R	0
M	0
,	0
l	0
=	0
1,	0
.	0
.	0
.	0
,	0
L,	0
are	0
L	0
random	0
hyper-	0
0	0
planes.	0
Then,	0
in	0
that	0
space	0
(R	0
M	0
L	0
),	0
S	0
3	0
H	0
seeks	0
a	0
0	0
direction	0
5	0
that	0
can	0
best	0
separate	0
the	0
data.	0
0	0
From	0
this	0
point	0
of	0
view,	0
it	0
is	0
obvious	0
that	0
the	0
basic	0
0	0
SH	0
is	0
a	0
special	0
case	0
of	0
S	0
3	0
H	0
when	0
w	0
is	0
set	0
to	0
e	0
=	0
0	0
[1,	0
1,	0
.	0
.	0
.	0
,	0
1].	0
That	0
is,	0
SH	0
firstly	0
maps	0
the	0
data	0
via	0
0	0
()	0
just	0
as	0
S	0
3	0
H.	0
But	0
then,	0
SH	0
directly	0
separates	0
the	0
0	0
data	0
in	0
that	0
feature	0
space	0
at	0
the	0
direction	0
e.	0
0	0
Analogously,	0
we	0
ignore	0
the	0
regularization	0
terms	0
0	0
in	0
SSH	0
and	0
rewrite	0
the	0
objective	0
of	0
SSH	0
as:	0
0	0
J(W)	0
SSH	0
=	0
0	0
1	0
0	0
2	0
0	0
tr[W	0
T	0
X(	0
+	0
)X	0
T	0
W]	0
(14)	0
0	0
where	0
W	0
=	0
[w	0
1	0
,	0
.	0
.	0
.	0
,	0
w	0
L	0
]	0
R	0
M	0
L	0
are	0
L	0
hyper-	0
0	0
planes	0
and	0
X	0
=	0
[x	0
1	0
,	0
.	0
.	0
.	0
,	0
x	0
N	0
].	0
Maximizing	0
this	0
ob-	0
0	0
jective	0
is	0
equivalent	0
to	0
maximizing	0
the	0
following:	0
0	0
0	0
J(W)	0
SSH	0
=	0
0	0
|	0
tr[W	0
T	0
S	0
+	0
W]|	0
0	0
|	0
tr[W	0
T	0
S	0
W]|	0
0	0
(15)	0
0	0
where	0
S	0
+	0
=	0
X	0
+	0
X	0
T	0
and	0
S	0
=	0
X	0
X	0
T	0
.	0
Equa-	0
0	0
tion	0
(15)	0
shows	0
that	0
SSH	0
is	0
analogous	0
to	0
Multiple	0
0	0
Discriminant	0
Analysis	0
(MDA)	0
(Duda	0
et	0
al.,	0
2000).	0
0	0
In	0
fact,	0
SSH	0
uses	0
top	0
L	0
best-separative	0
hyperplanes	0
0	0
in	0
the	0
original	0
attribute	0
space	0
found	0
via	0
PCA	0
to	0
hash	0
0	0
the	0
data.	0
Furthermore,	0
we	0
rewrite	0
the	0
projection	0
0	0
function	0
()	0
in	0
S	0
3	0
H	0
as:	0
0	0
(x)	0
=	0
x	0
[R	0
1	0
,	0
.	0
.	0
.	0
,	0
R	0
L	0
]	0
0	0
(16)	0
0	0
where	0
R	0
l	0
=	0
diag(sign(r	0
l	0
)).	0
Each	0
R	0
l	0
is	0
a	0
mapping	0
0	0
from	0
R	0
M	0
to	0
R	0
M	0
and	0
corresponds	0
to	0
one	0
embedding	0
0	0
space.	0
From	0
this	0
perspective,	0
unlike	0
SSH,	0
S	0
3	0
H	0
glob-	0
0	0
ally	0
seeks	0
a	0
direction	0
that	0
can	0
best	0
separate	0
the	0
data	0
0	0
in	0
L	0
different	0
embedding	0
spaces	0
simultaneously.	0
0	0
4	0
Experiments	0
0	0
We	0
use	0
two	0
datasets	0
20	0
Newsgroups	0
and	0
Open	0
Di-	0
0	0
rectory	0
Project	0
(ODP)	0
in	0
our	0
experiments.	0
Each	0
doc-	0
0	0
ument	0
is	0
represented	0
as	0
a	0
vector	0
of	0
occurrence	0
num-	0
0	0
bers	0
of	0
the	0
terms	0
within	0
it.	0
The	0
class	0
information	0
0	0
of	0
docs	0
is	0
considered	0
as	0
prior	0
knowledge	0
that	0
two	0
0	0
docs	0
within	0
a	0
same	0
class	0
should	0
have	0
more	0
similar	0
0	0
fingerprints	0
while	0
two	0
docs	0
within	0
different	0
classes	0
0	0
should	0
have	0
dissimilar	0
fingerprints.	0
We	0
will	0
demon-	0
0	0
strate	0
that	0
our	0
S	0
3	0
H	0
can	0
effectively	0
incorporate	0
this	0
0	0
prior	0
knowledge	0
to	0
improve	0
the	0
DSS	0
performance.	0
0	0
5	0
The	0
direction	0
is	0
determined	0
by	0
concatenating	0
w	0
L	0
times.	0
0	0
97	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
0.1	0
0	0
0.2	0
0	0
0.3	0
0	0
0.4	0
0	0
0.5	0
0	0
Mean	0
Averaged	0
Precision	0
(MAP)	0
0	0
Number	0
of	0
bits	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
(a)	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
0.1	0
0	0
0.2	0
0	0
0.3	0
0	0
0.4	0
0	0
0.5	0
0	0
Mean	0
Averaged	0
Precision	0
(MAP)	0
0	0
Number	0
of	0
bits	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
(b)	0
0	0
Figure	0
1:	0
Mean	0
Averaged	0
Precision	0
(MAP)	0
for	0
different	0
0	0
number	0
of	0
bits	0
for	0
hash	0
ranking	0
on	0
20	0
Newsgroups.	0
(a)	0
0	0
10K	0
features.	0
(b)	0
30K	0
features.	0
0	0
We	0
use	0
Inverted	0
List	0
(IL)	0
(Manning	0
et	0
al.,	0
2002)	0
0	0
as	0
the	0
baseline.	0
In	0
fact,	0
given	0
a	0
query	0
doc,	0
IL	0
re-	0
0	0
turns	0
all	0
the	0
docs	0
that	0
contain	0
any	0
term	0
within	0
it.	0
0	0
We	0
also	0
compare	0
our	0
method	0
with	0
three	0
state-of-	0
0	0
the-art	0
hashing	0
methods,	0
i.e.,	0
KLSH,	0
SSH	0
and	0
SH.	0
0	0
In	0
KLSH,	0
we	0
adopt	0
the	0
RBF	0
kernel	0
(x	0
i	0
,	0
x	0
j	0
)	0
=	0
0	0
exp(	0
0	0
x	0
i	0
x	0
j	0
2	0
0	0
2	0
0	0
2	0
0	0
),	0
where	0
the	0
scaling	0
factor	0
2	0
takes	0
0	0
0.5	0
and	0
the	0
other	0
two	0
parameters	0
p	0
and	0
t	0
are	0
set	0
to	0
0	0
be	0
500	0
and	0
50	0
respectively.	0
The	0
parameter	0
in	0
SSH	0
0	0
is	0
set	0
to	0
1.	0
For	0
S	0
3	0
H,	0
we	0
simply	0
set	0
the	0
parameters	0
1	0
0	0
and	0
2	0
in	0
Equation	0
(8)	0
to	0
4	0
and	0
0.5	0
respectively.	0
To	0
0	0
objectively	0
reflect	0
the	0
performance	0
of	0
S	0
3	0
H,	0
we	0
eval-	0
0	0
uate	0
our	0
S	0
3	0
H	0
with	0
and	0
without	0
Feature	0
Contribution	0
0	0
Calculation	0
algorithm	0
(FCC)	0
(Algorithm	0
1).	0
Specif-	0
0	0
ically,	0
FCC-free	0
S	0
3	0
H	0
(denoted	0
as	0
S	0
3	0
H	0
f	0
)	0
is	0
just	0
a	0
0	0
simplification	0
when	0
Gs	0
in	0
S	0
3	0
H	0
are	0
simply	0
set	0
to	0
Ds.	0
0	0
For	0
quantitative	0
evaluation,	0
as	0
in	0
literature	0
(Wang	0
0	0
et	0
al.,	0
2010b;	0
Mu	0
et	0
al.,	0
2009),	0
we	0
calculate	0
the	0
pre-	0
0	0
cision	0
under	0
two	0
scenarios:	0
hash	0
lookup	0
and	0
hash	0
0	0
ranking.	0
For	0
hash	0
lookup,	0
the	0
proportion	0
of	0
good	0
0	0
neighbors	0
(have	0
the	0
same	0
class	0
label	0
as	0
the	0
query)	0
0	0
among	0
the	0
searched	0
objects	0
within	0
a	0
given	0
Hamming	0
0	0
radius	0
is	0
calculated	0
as	0
precision.	0
Similarly	0
to	0
(Wang	0
0	0
et	0
al.,	0
2010b;	0
Weiss	0
et	0
al.,	0
2009),	0
for	0
a	0
query	0
doc-	0
0	0
ument,	0
if	0
no	0
neighbors	0
within	0
the	0
given	0
Hamming	0
0	0
radius	0
can	0
be	0
found,	0
it	0
is	0
considered	0
as	0
zero	0
preci-	0
0	0
sion.	0
Note	0
that,	0
the	0
precision	0
of	0
IL	0
is	0
the	0
propor-	0
0	0
tion	0
of	0
good	0
neighbors	0
among	0
the	0
whole	0
searched	0
0	0
objects.	0
For	0
hash	0
ranking,	0
all	0
the	0
objects	0
in	0
X	0
are	0
0	0
ranked	0
in	0
terms	0
of	0
their	0
Hamming	0
distance	0
from	0
the	0
0	0
query	0
document,	0
and	0
the	0
top	0
K	0
nearest	0
neighbors	0
0	0
are	0
returned	0
as	0
the	0
result.	0
Then,	0
Mean	0
Averaged	0
Pre-	0
0	0
cision	0
(MAP)	0
(Manning	0
et	0
al.,	0
2002)	0
is	0
calculated.	0
0	0
We	0
also	0
calculate	0
the	0
averaged	0
intra-	0
and	0
inter-	0
class	0
0	0
Hamming	0
distance	0
for	0
various	0
hashing	0
methods.	0
In-	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
0.00	0
0	0
0.05	0
0	0
0.10	0
0	0
0.15	0
0	0
0.20	0
0	0
0.25	0
0	0
0.30	0
0	0
0.35	0
0	0
0.40	0
0	0
Precision	0
within	0
Hamming	0
radius	0
3	0
0	0
Number	0
of	0
bits	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
(a)	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
0.00	0
0	0
0.05	0
0	0
0.10	0
0	0
0.15	0
0	0
0.20	0
0	0
0.25	0
0	0
0.30	0
0	0
0.35	0
0	0
0.40	0
0	0
Precision	0
within	0
Hamming	0
radius	0
3	0
0	0
Number	0
of	0
bits	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
(b)	0
0	0
Figure	0
2:	0
Precision	0
within	0
Hamming	0
radius	0
3	0
for	0
hash	0
0	0
lookup	0
on	0
20	0
Newsgroups.	0
(a)	0
10K	0
features.	0
(b)	0
30K	0
0	0
features.	0
0	0
tuitively,	0
a	0
good	0
hashing	0
method	0
should	0
have	0
small	0
0	0
intra-class	0
distance	0
while	0
large	0
inter-class	0
distance.	0
0	0
We	0
test	0
all	0
the	0
methods	0
on	0
a	0
PC	0
with	0
a	0
2.66	0
GHz	0
0	0
processor	0
and	0
12GB	0
RAM.	0
All	0
experiments	0
repeate	0
0	0
10	0
times	0
and	0
the	0
averaged	0
results	0
are	0
reported.	0
0	0
4.1	0
20	0
Newsgroups	0
0	0
20	0
Newsgroups	0
6	0
contains	0
20K	0
messages,	0
about	0
1K	0
0	0
messages	0
from	0
each	0
of	0
20	0
different	0
newsgroups.	0
0	0
The	0
entire	0
vocabulary	0
includes	0
62,061	0
words.	0
To	0
0	0
evaluate	0
the	0
performance	0
for	0
different	0
feature	0
di-	0
0	0
mensions,	0
we	0
use	0
Chi-squared	0
feature	0
selection	0
al-	0
0	0
gorithm	0
(Forman,	0
2003)	0
to	0
select	0
10K	0
and	0
30K	0
fea-	0
0	0
tures.	0
The	0
averaged	0
message	0
length	0
is	0
54.1	0
for	0
10K	0
0	0
features	0
and	0
116.2	0
for	0
30K	0
features.	0
We	0
randomly	0
0	0
select	0
4K	0
massages	0
as	0
the	0
test	0
set	0
and	0
the	0
remain	0
0	0
16K	0
as	0
the	0
training	0
set.	0
To	0
train	0
SSH	0
and	0
S	0
3	0
H,	0
0	0
from	0
the	0
training	0
set,	0
we	0
randomly	0
generate	0
40K	0
0	0
message-pairs	0
as	0
a	0
and	0
80K	0
message-pairs	0
as	0
r	0
.	0
0	0
For	0
hash	0
ranking,	0
Figure	0
1	0
shows	0
MAP	0
for	0
vari-	0
0	0
ous	0
methods	0
using	0
different	0
number	0
of	0
bits.	0
It	0
shows	0
0	0
that	0
performance	0
of	0
SSH	0
decreases	0
with	0
the	0
grow-	0
0	0
ing	0
of	0
hash	0
bits.	0
This	0
is	0
mainly	0
because	0
the	0
variance	0
0	0
of	0
the	0
directions	0
obtained	0
by	0
PCA	0
decreases	0
with	0
0	0
the	0
decrease	0
of	0
their	0
ranks.	0
Thus,	0
lower	0
bits	0
have	0
0	0
larger	0
empirical	0
errors.	0
For	0
S	0
3	0
H,	0
FCC	0
(Algorithm	0
1)	0
0	0
can	0
significantly	0
improve	0
the	0
MAP	0
just	0
as	0
discussed	0
0	0
in	0
Section	0
3.2.	0
Moreover,	0
the	0
MAP	0
of	0
FCC-free	0
0	0
S	0
3	0
H	0
(S	0
3	0
H	0
f	0
)	0
is	0
affected	0
by	0
feature	0
dimensions	0
while	0
0	0
FCC-based	0
(S	0
3	0
H)	0
is	0
relatively	0
stable.	0
This	0
implies	0
0	0
FCC	0
can	0
also	0
improve	0
the	0
satiability	0
of	0
S	0
3	0
H.	0
As	0
we	0
0	0
see,	0
S	0
3	0
H	0
f	0
ignores	0
the	0
contribution	0
of	0
features	0
to	0
dif-	0
0	0
ferent	0
classes.	0
However,	0
besides	0
the	0
local	0
descrip-	0
0	0
tion	0
of	0
data	0
locality	0
in	0
the	0
form	0
of	0
object-pairs,	0
such	0
0	0
6	0
http://www.cs.cmu.edu/afs/cs/project/theo-3/www/	1
0	0
98	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
10	0
0	0
-1	0
0	0
10	0
0	0
0	0
0	0
10	0
0	0
1	0
0	0
10	0
0	0
2	0
0	0
10	0
0	0
3	0
0	0
10	0
0	0
4	0
0	0
Number	0
of	0
searched	0
data	0
0	0
Number	0
of	0
bits	0
0	0
(a)	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
10	0
0	0
-1	0
0	0
10	0
0	0
0	0
0	0
10	0
0	0
1	0
0	0
10	0
0	0
2	0
0	0
10	0
0	0
3	0
0	0
10	0
0	0
4	0
0	0
Number	0
of	0
searched	0
data	0
0	0
Number	0
of	0
bits	0
0	0
(b)	0
0	0
Figure	0
3:	0
Averaged	0
searched	0
sample	0
numbers	0
using	0
4K	0
0	0
query	0
messages	0
for	0
hash	0
lookup.	0
(a)	0
10K	0
features.	0
(b)	0
0	0
30K	0
features.	0
0	0
(global)	0
information	0
also	0
provides	0
a	0
proper	0
guidance	0
0	0
for	0
hashing.	0
So,	0
for	0
S	0
3	0
H	0
f	0
,	0
the	0
reason	0
why	0
its	0
re-	0
0	0
sults	0
with	0
30K	0
features	0
are	0
worse	0
than	0
the	0
results	0
0	0
with	0
10K	0
features	0
is	0
probably	0
because	0
S	0
3	0
H	0
f	0
learns	0
0	0
to	0
hash	0
only	0
according	0
to	0
the	0
local	0
description	0
of	0
0	0
data	0
locality	0
and	0
many	0
not	0
too	0
relevant	0
features	0
lead	0
0	0
to	0
relatively	0
poor	0
description.	0
In	0
contrast,	0
S	0
3	0
H	0
can	0
0	0
utilize	0
global	0
information	0
to	0
better	0
understand	0
the	0
0	0
similarity	0
among	0
objects.	0
In	0
short,	0
S	0
3	0
H	0
obtains	0
the	0
0	0
best	0
MAP	0
for	0
all	0
bits	0
and	0
feature	0
dimensions.	0
0	0
For	0
hash	0
lookup,	0
Figure	0
2	0
presents	0
the	0
precision	0
0	0
within	0
Hamming	0
radius	0
3	0
for	0
different	0
number	0
of	0
0	0
bits.	0
It	0
shows	0
that	0
IL	0
even	0
outperforms	0
SH.	0
This	0
0	0
is	0
because	0
few	0
objects	0
can	0
be	0
hashed	0
by	0
SH	0
into	0
one	0
0	0
hash	0
bucket.	0
Thus,	0
for	0
many	0
queries,	0
SH	0
fails	0
to	0
0	0
return	0
any	0
neighbor	0
even	0
in	0
a	0
large	0
Hamming	0
radius	0
0	0
of	0
3.	0
Clearly,	0
S	0
3	0
H	0
outperforms	0
all	0
the	0
other	0
methods	0
0	0
for	0
different	0
number	0
of	0
hash	0
bits	0
and	0
features.	0
0	0
The	0
number	0
of	0
messages	0
searched	0
by	0
different	0
0	0
methods	0
are	0
reported	0
in	0
Figure	0
3.	0
We	0
find	0
that	0
the	0
0	0
number	0
of	0
searched	0
data	0
of	0
S	0
3	0
H	0
(with/without	0
FCC)	0
0	0
decreases	0
much	0
more	0
slowly	0
than	0
KLSH,	0
SH	0
and	0
0	0
SSH	0
with	0
the	0
growing	0
of	0
the	0
number	0
of	0
hash	0
bits.	0
As	0
0	0
discussed	0
in	0
Section	0
3.4,	0
this	0
mainly	0
benefits	0
from	0
0	0
the	0
design	0
of	0
S	0
3	0
H	0
that	0
S	0
3	0
H	0
(globally)	0
seeks	0
a	0
di-	0
0	0
rection	0
that	0
can	0
best	0
separate	0
the	0
data	0
in	0
L	0
embed-	0
0	0
ding	0
spaces	0
simultaneously.	0
We	0
also	0
find	0
IL	0
returns	0
0	0
a	0
large	0
number	0
of	0
neighbors	0
of	0
each	0
query	0
message	0
0	0
which	0
leads	0
to	0
its	0
poor	0
efficiency.	0
0	0
The	0
averaged	0
intra-	0
and	0
inter-	0
class	0
Hamming	0
dis-	0
0	0
tance	0
of	0
different	0
methods	0
are	0
reported	0
in	0
Table	0
1.	0
0	0
As	0
it	0
shows,	0
S	0
3	0
H	0
has	0
relatively	0
larger	0
margin	0
()	0
0	0
between	0
intra-	0
and	0
inter-class	0
Hamming	0
distance.	0
0	0
This	0
indicates	0
that	0
S	0
3	0
H	0
is	0
more	0
effective	0
to	0
make	0
0	0
similar	0
points	0
have	0
similar	0
fingerprints	0
while	0
keep	0
0	0
intra-class	0
0	0
inter-class	0
0	0
0	0
S	0
3	0
H	0
0	0
13.1264	0
0	0
15.6342	0
0	0
2.5078	0
0	0
S	0
3	0
H	0
f	0
0	0
12.5754	0
0	0
13.3479	0
0	0
0.7725	0
0	0
SSH	0
0	0
6.4134	0
0	0
6.5262	0
0	0
0.1128	0
0	0
SH	0
0	0
15.3908	0
0	0
15.6339	0
0	0
0.2431	0
0	0
KLSH	0
0	0
10.2876	0
0	0
10.8713	0
0	0
0.5841	0
0	0
Table	0
1:	0
Averaged	0
intra-	0
and	0
inter-	0
class	0
Hamming	0
dis-	0
0	0
tance	0
of	0
20	0
Newsgroups	0
for	0
32-bit	0
fingerprint.	0
is	0
the	0
0	0
difference	0
between	0
the	0
averaged	0
inter-	0
and	0
intra-	0
class	0
0	0
Hamming	0
distance.	0
Large	0
implies	0
good	0
hashing.	0
0	0
10	0
0	0
20	0
0	0
30	0
0	0
40	0
0	0
10	0
0	0
0	0
10	0
1	0
0	0
10	0
2	0
0	0
10	0
3	0
0	0
Time	0
(sec.)	0
0	0
Feature	0
dimension	0
(K)	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
(a)	0
0	0
10	0
0	0
20	0
0	0
30	0
0	0
40	0
0	0
10	0
1	0
0	0
10	0
2	0
0	0
10	0
3	0
0	0
10	0
4	0
0	0
Space	0
(MB)	0
0	0
Feature	0
dimension	0
(K)	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
SSH	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
(b)	0
0	0
Figure	0
4:	0
Computational	0
complexity	0
of	0
training	0
for	0
dif-	0
0	0
ferent	0
feature	0
dimensions	0
for	0
32-bit	0
fingerprint.	0
(a)	0
Train-	0
0	0
ing	0
time	0
(sec).	0
(b)	0
Training	0
space	0
cost	0
(MB).	0
0	0
the	0
dissimilar	0
points	0
away	0
enough	0
from	0
each	0
other.	0
0	0
Figure	0
4	0
shows	0
the	0
(training)	0
computational	0
com-	0
0	0
plexity	0
of	0
different	0
methods.	0
We	0
find	0
that	0
the	0
time	0
0	0
and	0
space	0
cost	0
of	0
SSH	0
grows	0
much	0
faster	0
than	0
SH,	0
0	0
KLSH	0
and	0
S	0
3	0
H	0
with	0
the	0
growing	0
of	0
feature	0
dimen-	0
0	0
sion.	0
This	0
is	0
mainly	0
because	0
SSH	0
requires	0
SVD	0
to	0
0	0
find	0
the	0
optimal	0
hashing	0
functions	0
which	0
is	0
compu-	0
0	0
tational	0
expensive.	0
Instead,	0
S	0
3	0
H	0
seeks	0
the	0
optimal	0
0	0
feature	0
weights	0
via	0
L-BFGS,	0
which	0
is	0
still	0
efficient	0
0	0
even	0
for	0
very	0
high-dimensional	0
data.	0
0	0
4.2	0
Open	0
Directory	0
Project	0
(ODP)	0
0	0
Open	0
Directory	0
Project	0
(ODP)	0
7	0
is	0
a	0
multilingual	0
0	0
open	0
content	0
directory	0
of	0
web	0
links	0
(docs)	0
organized	0
0	0
by	0
a	0
hierarchical	0
ontology	0
scheme.	0
In	0
our	0
exper-	0
0	0
iment,	0
only	0
English	0
docs	0
8	0
at	0
level	0
3	0
of	0
the	0
cate-	0
0	0
gory	0
tree	0
are	0
utilized	0
to	0
evaluate	0
the	0
performance.	0
0	0
In	0
short,	0
the	0
dataset	0
contains	0
2,483,388	0
docs	0
within	0
0	0
6,008	0
classes.	0
There	0
are	0
totally	0
862,050	0
distinct	0
0	0
words	0
and	0
each	0
doc	0
contains	0
14.13	0
terms	0
on	0
aver-	0
0	0
age.	0
Since	0
docs	0
are	0
too	0
short,	0
we	0
do	0
not	0
conduct	0
0	0
7	0
http://rdf.dmoz.org/	1
0	0
8	0
The	0
title	0
together	0
with	0
the	0
corresponding	0
short	0
description	0
0	0
of	0
a	0
page	0
are	0
considered	0
as	0
a	0
document	0
in	0
our	0
experiments.	0
0	0
99	0
0	0
1	0
0	0
10	0
0	0
100	0
0	0
1k	0
0	0
10k	0
0	0
100k	0
0	0
0.00	0
0	0
0.01	0
0	0
0.02	0
0	0
0.03	0
0	0
0.04	0
0	0
Percentage	0
0	0
Class	0
size	0
0	0
(a)	0
0	0
0	0
0	0
20	0
0	0
40	0
0	0
60	0
0	0
80	0
0	0
100	0
0	0
120	0
0	0
0.00	0
0	0
0.02	0
0	0
0.04	0
0	0
0.06	0
0	0
0.08	0
0	0
0.10	0
0	0
Percentage	0
0	0
Document	0
length	0
0	0
(b)	0
0	0
Figure	0
5:	0
Overview	0
of	0
ODP	0
data	0
set.	0
(a)	0
Class	0
distribu-	0
0	0
tion	0
at	0
level	0
3.	0
(b)	0
Distribution	0
of	0
document	0
length.	0
0	0
intra-class	0
0	0
inter-class	0
0	0
0	0
S	0
3	0
H	0
0	0
14.0029	0
0	0
15.9508	0
0	0
1.9479	0
0	0
S	0
3	0
H	0
f	0
0	0
14.3801	0
0	0
15.5260	0
0	0
1.1459	0
0	0
SH	0
0	0
14.7725	0
0	0
15.6432	0
0	0
0.8707	0
0	0
KLSH	0
0	0
9.3382	0
0	0
10.5700	0
0	0
1.2328	0
0	0
Table	0
2:	0
Averaged	0
intra-	0
and	0
inter-	0
class	0
Hamming	0
dis-	0
0	0
tance	0
of	0
ODP	0
for	0
32-bit	0
fingerprint	0
(860K	0
features).	0
0	0
is	0
the	0
difference	0
between	0
averaged	0
intra-	0
and	0
inter-	0
class	0
0	0
Hamming	0
distance.	0
0	0
feature	0
selection	0
9	0
.	0
An	0
overview	0
of	0
ODP	0
is	0
shown	0
in	0
0	0
Figure	0
5.	0
We	0
randomly	0
sample	0
10%	0
docs	0
as	0
the	0
test	0
0	0
set	0
and	0
the	0
remain	0
as	0
the	0
training	0
set.	0
Furthermore,	0
0	0
from	0
training	0
set,	0
we	0
randomly	0
generate	0
800K	0
doc-	0
0	0
pairs	0
as	0
a	0
,	0
and	0
1	0
million	0
doc-pairs	0
as	0
r	0
.	0
Note	0
0	0
that,	0
since	0
there	0
are	0
totally	0
over	0
800K	0
features,	0
it	0
0	0
is	0
extremely	0
inefficient	0
to	0
train	0
SSH.	0
Therefore,	0
we	0
0	0
only	0
compare	0
our	0
S	0
3	0
H	0
with	0
IL,	0
KLSH	0
and	0
SH.	0
0	0
The	0
search	0
performance	0
is	0
given	0
in	0
Figure	0
6.	0
Fig-	0
0	0
ure	0
6(a)	0
shows	0
the	0
MAP	0
for	0
various	0
methods	0
using	0
0	0
different	0
number	0
of	0
bits.	0
It	0
shows	0
KLSH	0
outper-	0
0	0
forms	0
SH,	0
which	0
mainly	0
contributes	0
to	0
the	0
kernel	0
0	0
trick.	0
S	0
3	0
H	0
and	0
S	0
3	0
H	0
f	0
have	0
higher	0
MAP	0
than	0
KLSH	0
0	0
and	0
SH.	0
Clearly,	0
FCC	0
algorithm	0
can	0
improve	0
the	0
0	0
MAP	0
of	0
S	0
3	0
H	0
for	0
all	0
bits.	0
Figure	0
6(b)	0
presents	0
the	0
0	0
precision	0
within	0
Hamming	0
radius	0
2	0
for	0
hash	0
lookup.	0
0	0
We	0
find	0
that	0
IL	0
outperforms	0
SH	0
since	0
SH	0
fails	0
for	0
0	0
many	0
queries.	0
It	0
also	0
shows	0
that	0
S	0
3	0
H	0
(with	0
FCC)	0
0	0
can	0
obtain	0
the	0
best	0
precision	0
for	0
all	0
bits.	0
0	0
Table	0
2	0
reports	0
the	0
averaged	0
intra-	0
and	0
inter-class	0
0	0
Hamming	0
distance	0
for	0
various	0
methods.	0
It	0
shows	0
0	0
that	0
S	0
3	0
H	0
has	0
the	0
largest	0
margin	0
().	0
This	0
demon-	0
0	0
9	0
We	0
have	0
tested	0
feature	0
selection.	0
However,	0
if	0
we	0
select	0
0	0
40K	0
features	0
via	0
Chi-squared	0
feature	0
selection	0
method,	0
docu-	0
0	0
ments	0
are	0
represented	0
by	0
3.15	0
terms	0
on	0
average.	0
About	0
44.9%	0
0	0
documents	0
are	0
represented	0
by	0
no	0
more	0
than	0
2	0
terms.	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
0.15	0
0	0
0.20	0
0	0
0.25	0
0	0
0.30	0
0	0
0.35	0
0	0
Mean	0
Averaged	0
Precision	0
(MAP)	0
0	0
Number	0
of	0
bits	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SH	0
0	0
KLSH	0
0	0
(a)	0
0	0
24	0
0	0
32	0
0	0
40	0
0	0
48	0
0	0
56	0
0	0
64	0
0	0
0.03	0
0	0
0.06	0
0	0
0.09	0
0	0
0.12	0
0	0
0.15	0
0	0
0.18	0
0	0
Precision	0
within	0
Hamming	0
radius	0
2	0
0	0
Number	0
of	0
bits	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
S	0
0	0
3	0
0	0
H	0
0	0
f	0
0	0
SH	0
0	0
KLSH	0
0	0
IL	0
0	0
(b)	0
0	0
Figure	0
6:	0
Retrieval	0
performance	0
of	0
different	0
methods	0
on	0
0	0
ODP.	0
(a)	0
Mean	0
Averaged	0
Precision	0
(MAP)	0
for	0
different	0
0	0
number	0
of	0
bits	0
for	0
hash	0
ranking.	0
(b)	0
Precision	0
within	0
0	0
Hamming	0
radius	0
2	0
for	0
hash	0
lookup.	0
0	0
strates	0
S	0
3	0
H	0
can	0
measure	0
the	0
similarity	0
among	0
the	0
0	0
data	0
better	0
than	0
KLSH	0
and	0
SH.	0
0	0
We	0
should	0
emphasize	0
that	0
KLSH	0
needs	0
0.3ms	0
0	0
to	0
return	0
the	0
results	0
for	0
a	0
query	0
document	0
for	0
hash	0
0	0
lookup,	0
and	0
S	0
3	0
H	0
needs	0
<0.1ms.	0
In	0
contrast,	0
IL	0
re-	0
0	0
quires	0
about	0
75ms	0
to	0
finish	0
searching.	0
This	0
is	0
mainly	0
0	0
because	0
IL	0
always	0
returns	0
a	0
large	0
number	0
of	0
ob-	0
0	0
jects	0
(dozens	0
or	0
hundreds	0
times	0
more	0
than	0
S	0
3	0
H	0
and	0
0	0
KLSH)	0
and	0
requires	0
much	0
time	0
for	0
post-processing.	0
0	0
All	0
the	0
experiments	0
show	0
S	0
3	0
H	0
is	0
more	0
effective,	0
0	0
efficient	0
and	0
stable	0
than	0
the	0
baseline	0
method	0
and	0
the	0
0	0
state-of-the-art	0
hashing	0
methods.	0
0	0
5	0
Conclusions	0
0	0
We	0
have	0
proposed	0
a	0
novel	0
supervised	0
hashing	0
0	0
method	0
named	0
Semi-Supervised	0
Simhash	0
(S	0
3	0
H)	0
for	0
0	0
high-dimensional	0
data	0
similarity	0
search.	0
S	0
3	0
H	0
learns	0
0	0
the	0
optimal	0
feature	0
weights	0
from	0
prior	0
knowledge	0
0	0
to	0
relocate	0
the	0
data	0
such	0
that	0
similar	0
objects	0
have	0
0	0
similar	0
fingerprints.	0
This	0
is	0
implemented	0
by	0
max-	0
0	0
imizing	0
the	0
empirical	0
accuracy	0
on	0
labeled	0
data	0
to-	0
0	0
gether	0
with	0
the	0
entropy	0
of	0
hash	0
functions.	0
The	0
0	0
proposed	0
method	0
leads	0
to	0
a	0
simple	0
Quasi-Newton	0
0	0
based	0
solution	0
which	0
is	0
efficient	0
even	0
for	0
very	0
high-	0
0	0
dimensional	0
data.	0
Experiments	0
performed	0
on	0
two	0
0	0
large	0
datasets	0
have	0
shown	0
that	0
S	0
3	0
H	0
has	0
better	0
search	0
0	0
performance	0
than	0
several	0
state-of-the-art	0
methods.	0
0	0
6	0
Acknowledgements	0
0	0
We	0
thank	0
Fangtao	0
Li	0
for	0
his	0
insightful	0
suggestions.	0
0	0
We	0
would	0
also	0
like	0
to	0
thank	0
the	0
anonymous	0
review-	0
0	0
ers	0
for	0
their	0
helpful	0
comments.	0
This	0
work	0
is	0
sup-	0
0	0
ported	0
by	0
the	0
National	0
Natural	0
Science	0
Foundation	0
0	0
of	0
China	0
under	0
Grant	0
No.	0
60873174.	0
0	0
100	0
0	0
References	0
0	0
Christopher	0
J.C.	0
Burges.	0
1998.	0
A	0
tutorial	0
on	0
support	0
0	0
vector	0
machines	0
for	0
pattern	0
recognition.	0
Data	0
Mining	0
0	0
and	0
Knowledge	0
Discovery,	0
2(2):121-167.	0
0	0
Moses	0
S.	0
Charikar.	0
2002.	0
Similarity	0
estimation	0
tech-	0
0	0
niques	0
from	0
rounding	0
algorithms.	0
In	0
Proceedings	0
0	0
of	0
the	0
34th	0
annual	0
ACM	0
symposium	0
on	0
Theory	0
of	0
0	0
computing,	0
pages	0
380-388.	0
0	0
Gianni	0
Costa,	0
Giuseppe	0
Manco	0
and	0
Riccardo	0
Ortale.	0
0	0
2010.	0
An	0
incremental	0
clustering	0
scheme	0
for	0
data	0
de-	0
0	0
duplication.	0
Data	0
Mining	0
and	0
Knowledge	0
Discovery,	0
0	0
20(1):152-187.	0
0	0
Jeffrey	0
Dean	0
and	0
Monika	0
R.	0
Henzinge.	0
1999.	0
Finding	0
0	0
Related	0
Pages	0
in	0
the	0
World	0
Wide	0
Web.	0
Computer	0
0	0
Networks,	0
31:1467-1479.	0
0	0
Richard	0
O.	0
Duda,	0
Peter	0
E.	0
Hart	0
and	0
David	0
G.	0
Stork.	0
0	0
2000.	0
Pattern	0
classification,	0
2nd	0
edition.	0
Wiley-	0
0	0
Interscience.	0
0	0
George	0
Forman	0
2003.	0
An	0
extensive	0
empirical	0
study	0
of	0
0	0
feature	0
selection	0
metrics	0
for	0
text	0
classification.	0
The	0
0	0
Journal	0
of	0
Machine	0
Learning	0
Research,	0
3:1289-1305.	0
0	0
Piotr	0
Indyk	0
and	0
Rajeev	0
Motwani.	0
1998.	0
Approximate	0
0	0
nearest	0
neighbors:	0
towards	0
removing	0
the	0
curse	0
of	0
0	0
dimensionality.	0
In	0
Proceedings	0
of	0
the	0
30th	0
annual	0
0	0
ACM	0
symposium	0
on	0
Theory	0
of	0
computing,	0
pages	0
0	0
604-613.	0
0	0
Ian	0
Jolliffe.	0
1986.	0
Principal	0
Component	0
Analysis.	0
0	0
Springer-Verlag,	0
New	0
York.	0
0	0
Yan	0
Ke,	0
Rahul	0
Sukthankar	0
and	0
Larry	0
Huston.	0
2004.	0
0	0
Efficient	0
near-duplicate	0
detection	0
and	0
sub-image	0
0	0
retrieval.	0
In	0
Proceedings	0
of	0
the	0
ACM	0
International	0
0	0
Conference	0
on	0
Multimedia.	0
0	0
Brian	0
Kulis	0
and	0
Kristen	0
Grauman.	0
2009.	0
Kernelized	0
0	0
locality-sensitive	0
hashing	0
for	0
scalable	0
image	0
search.	0
0	0
In	0
Proceedings	0
of	0
the	0
12th	0
International	0
Conference	0
0	0
on	0
Computer	0
Vision,	0
pages	0
2130-2137.	0
0	0
Brian	0
Kulis,	0
Prateek	0
Jain	0
and	0
Kristen	0
Grauman.	0
2009.	0
0	0
Fast	0
similarity	0
search	0
for	0
learned	0
metrics.	0
IEEE	0
0	0
Transactions	0
on	0
Pattern	0
Analysis	0
and	0
Machine	0
Intelli-	0
0	0
gence,	0
pages	0
2143-2157.	0
0	0
Dong	0
C.	0
Liu	0
and	0
Jorge	0
Nocedal.	0
1989.	0
On	0
the	0
limited	0
0	0
memory	0
BFGS	0
method	0
for	0
large	0
scale	0
optimization.	0
0	0
Mathematical	0
programming,	0
45(1):	0
503-528.	0
0	0
Omid	0
Madani,	0
Michael	0
Connor	0
and	0
Wiley	0
Greiner.	0
0	0
2009.	0
Learning	0
when	0
concepts	0
abound.	0
The	0
Journal	0
0	0
of	0
Machine	0
Learning	0
Research,	0
10:2571-2613.	0
0	0
Gurmeet	0
Singh	0
Manku,	0
Arvind	0
Jain	0
and	0
Anish	0
Das	0
0	0
Sarma.	0
2007.	0
Detecting	0
near-duplicates	0
for	0
web	0
0	0
crawling.	0
In	0
Proceedings	0
of	0
the	0
16th	0
international	0
0	0
conference	0
on	0
World	0
Wide	0
Web,	0
pages	0
141-150.	0
0	0
Christopher	0
D.	0
Manning,	0
Prabhakar	0
Raghavan	0
and	0
Hin-	0
0	0
rich	0
Sch	0
utze.	0
2002.	0
An	0
introduction	0
to	0
information	0
0	0
retrieval.	0
Spring.	0
0	0
Yadong	0
Mu,	0
Jialie	0
Shen	0
and	0
Shuicheng	0
Yan.	0
2010.	0
0	0
Weakly-Supervised	0
Hashing	0
in	0
Kernel	0
Space.	0
In	0
Pro-	0
0	0
ceedings	0
of	0
International	0
Conference	0
on	0
Computer	0
0	0
Vision	0
and	0
Pattern	0
Recognition,	0
pages	0
3344-3351.	0
0	0
Ruslan	0
Salakhutdinov	0
and	0
Geoffrey	0
Hintona.	0
2007.	0
0	0
Semantic	0
hashing.	0
In	0
SIGIR	0
workshop	0
on	0
Information	0
0	0
Retrieval	0
and	0
applications	0
of	0
Graphical	0
Models.	0
0	0
Bernhard	0
Sch	0
olkopf,	0
Alexander	0
Smola	0
and	0
Klaus-Robert	0
0	0
M	0
uller.	0
1997.	0
Kernel	0
principal	0
component	0
analysis.	0
0	0
Advances	0
in	0
Kernel	0
Methods	0
-	0
Support	0
Vector	0
Learn-	0
0	0
ing,	0
pages	0
583-588.	0
MIT.	0
0	0
Lloyd	0
N.	0
Trefethen	0
and	0
David	0
Bau.	0
1997.	0
Numerical	0
0	0
linear	0
algebra.	0
Society	0
for	0
Industrial	0
Mathematics.	0
0	0
Xiaojun	0
Wan,	0
Jianwu	0
Yang	0
and	0
Jianguo	0
Xiao.	0
2008.	0
0	0
Towards	0
a	0
unified	0
approach	0
to	0
document	0
similarity	0
0	0
search	0
using	0
manifold-ranking	0
of	0
blocks.	0
Information	0
0	0
Processing	0
&	0
Management,	0
44(3):1032-1048.	0
0	0
Jun	0
Wang,	0
Sanjiv	0
Kumar	0
and	0
Shih-Fu	0
Chang.	0
2010a.	0
0	0
Semi-Supervised	0
Hashing	0
for	0
Scalable	0
Image	0
Re-	0
0	0
trieval.	0
In	0
Proceedings	0
of	0
International	0
Conference	0
0	0
on	0
Computer	0
Vision	0
and	0
Pattern	0
Recognition,	0
pages	0
0	0
3424-3431.	0
0	0
Jun	0
Wang,	0
Sanjiv	0
Kumar	0
and	0
Shih-Fu	0
Chang.	0
2010b.	0
0	0
Sequential	0
Projection	0
Learning	0
for	0
Hashing	0
with	0
0	0
Compact	0
Codes.	0
In	0
Proceedings	0
of	0
International	0
0	0
Conference	0
on	0
Machine	0
Learning.	0
0	0
Yair	0
Weiss,	0
Antonio	0
Torralba	0
and	0
Rob	0
Fergus.	0
2009.	0
0	0
Spectral	0
hashing.	0
In	0
Proceedings	0
of	0
Advances	0
in	0
Neu-	0
0	0
ral	0
Information	0
Processing	0
Systems.	0
0	0
101	0
0	0
