xxx	0	0	0.0	0	0	0	0
Learning	to	0	0.0	1.43	2	4	0
$$$	Abstract	0	0.0	1.2	5	6	0
We	study	10	0.0	1.0	2	4	0
1	Introduction	0	0.0	1.2	1	2	0
Peer	review	10	0.0	1.0	2	4	0
semantic	indexing	8	0.0	1.0	4	4	0
The	problem	6	0.0	1.0	2	4	0
In	this	9	0.0	1.0	2	4	0
To	summarize,	0	0.0	1.0	2	3	0
Formalization	of	0	0.0	1.0	2	4	0
Proposal	of	1	0.0	1.0	2	4	0
Empirical	validation	1	0.0	1.0	2	4	0
2	Problem	0	0.0	1.2	1	2	0
Given	a	7	0.0	0.99	2	4	0
Problem	1	3	0.0	1.0	2	1	0
where	Y	0	0.0	0.94	4	1	0
,	,	0	0.0	0.92	3	3	0
}	represents	2	0.0	0.98	3	4	0
The	predictive	2	0.0	1.0	2	4	0
3	Approach	0	0.0	1.2	1	2	0
The	common	4	0.0	1.0	2	4	0
Candidate	Generation.	0	0.0	0.97	2	2	0
,	we	0	0.0	1.0	3	4	0
Declination	Predication.	2	0.0	1.0	2	2	0
Interactive	Learning.	1	0.0	1.0	2	2	0
3.1	Candidate	0	0.0	1.09	1	2	0
Given	a	6	0.0	1.0	2	4	0
P	(w|d)	0	0.0	0.92	1	3	0
tf	(w,	0	0.0	1.0	4	3	0
N	d	0	0.0	0.92	1	4	0
N	D	0	0.0	0.9	1	1	0
where	N	0	0.0	0.9	4	1	0
is	the	8	0.0	0.99	4	4	0
3.2	Declination	0	0.0	1.09	1	2	0
After	retrieving	11	0.0	0.99	2	4	0
#	reviewers	0	0.0	0.54	3	4	0
$$$	Decline	0	0.0	0.57	5	2	0
$$$	percentage	0	0.0	0.57	5	4	0
0	0.1	0	0.0	0.52	4	4	0
Figure	1:	1	0.0	1.0	0	3	0
More	specifically,	2	0.0	1.0	2	3	0
f	(p,	0	0.0	0.94	4	3	0
Z	a	0	0.0	0.9	1	4	0
k	k	0	0.0	0.86	4	4	0
where	k	0	0.0	0.85	4	4	0
(.)	is	0	0.0	0.94	3	4	0
feature	function	0	0.0	0.96	4	4	0
with	respect	2	0.0	0.97	4	4	0
g(y	i	0	0.0	0.92	3	1	0
Z	b	0	0.0	0.9	1	4	0
l	l	0	0.0	0.85	4	4	0
where	i	0	0.0	0.85	4	1	0
(.)	is	0	0.0	0.94	3	4	0
feature	function	0	0.0	0.95	4	4	0
and	y	0	0.0	0.9	4	4	0
;	l	0	0.0	0.85	3	4	0
is	the	2	0.0	0.99	4	4	0
log	P	0	0.0	1.0	4	1	0
yiY	k	0	0.0	0.83	4	4	0
$$$	+	0	0.0	1.0	5	3	0
vivj	l	0	0.0	0.86	4	4	0
where	Z	0	0.0	0.94	4	1	0
Z	b	0	0.0	0.85	1	4	0
is	the	0	0.0	0.95	4	4	0
v	j	0	0.0	0.85	1	4	0
indi-	cates	1	0.0	0.97	3	4	0
Feature	Definitions	0	0.0	1.0	2	2	0
Now	we	0	0.0	0.97	2	4	0
(p,	v	0	0.0	0.9	3	1	0
,	y	4	0.0	0.98	3	4	0
Basic	statistics.	1	0.0	1.0	2	4	0
Expertise	matching.	3	0.0	1.0	2	4	0
Organization.	We	1	0.0	1.0	2	2	0
Two	correlation	2	0.0	1.0	2	4	0
Model	learning	2	0.0	1.0	2	4	0
This	optimization	1	0.0	1.0	2	4	0
i	=	0	0.0	0.85	1	3	0
$$$	j	0	0.0	0.7	5	4	0
f	(x	0	0.0	0.92	4	3	0
$$$	Z()	0	0.0	1.0	5	2	0
=	E[(x	1	0.0	0.85	3	2	0
$$$	(7)	0	0.0	1.0	5	3	0
In	this	13	0.0	0.99	2	4	0
vf	(x	0	0.0	0.88	4	3	0
f	N	0	0.0	0.79	4	1	0
f	v	0	0.0	0.85	4	1	0
$$$	xv	0	0.0	0.7	5	1	0
f	(x	0	0.0	0.92	4	3	0
v	N	0	0.0	0.8	1	1	0
Algorithm	1	0	0.0	1.0	2	1	0
Input:	Query	0	0.0	0.97	2	2	0
},	G	0	0.0	1.0	3	1	0
0;	repeat	0	0.0	0.96	3	4	0
do	L	3	0.0	0.96	4	1	0
where	N	9	0.0	0.99	4	1	0
Declination	Prediction	0	0.0	1.0	2	2	0
Given	the	2	0.0	0.99	2	4	0
Y	p	0	0.0	0.92	1	4	0
For	inference,	0	0.0	0.97	2	3	0
that	maximize	2	0.0	1.0	4	4	0
3.3	Interactive	0	0.0	1.09	1	2	0
In	our	3	0.0	1.0	2	4	0
The	technical	3	0.0	1.0	2	4	0
E	new	0	0.0	0.94	1	4	0
N	+	0	0.0	0.96	1	3	0
N	+	0	0.0	0.84	1	3	0
where	{	0	0.0	0.85	4	3	0
(x	N	0	0.0	0.8	3	1	0
,	y	0	0.0	0.85	3	4	0
)}	denotes	15	0.0	0.98	3	4	0
4	Experimental	0	0.0	1.2	1	2	0
To	empirically	2	0.0	1.0	2	4	0
4.1	Experimental	0	0.0	1.09	1	2	0
$$$	Datasets	0	0.0	1.0	5	2	0
We	evaluate	8	0.0	1.0	2	4	0
SIGGRAPH,	and	7	0.0	1.0	2	4	0
Comparison	Methods	0	0.0	1.0	2	2	0
We	compare	3	0.0	1.0	2	4	0
Sim(p,	r)	0	0.0	0.9	2	3	0
where	K	0	0.0	0.9	4	1	0
is	the	0	0.0	0.97	4	4	0
is	the	4	0.0	1.0	4	4	0
Evaluation	Measures	0	0.0	1.0	2	2	0
In	all	5	0.0	1.0	2	4	0
4.2	Performance	0	0.0	1.09	1	2	0
We	compare	4	0.0	1.0	2	4	0
Table	1:	0	0.0	1.0	0	3	0
Method	P@5	0	0.0	1.0	2	2	0
Content	64.1	0	0.0	1.0	2	4	0
SVM-Rank	64.7	0	0.0	1.0	2	4	0
RankFG	60.0	0	0.0	1.0	2	4	0
Table	2:	0	0.0	1.0	0	3	0
Method	P@1	0	0.0	1.0	2	2	0
Content	55.9	0	0.0	1.0	2	4	0
SVM-Rank	52.9	0	0.0	1.0	2	4	0
RankFG	61.8	0	0.0	1.0	2	4	0
on	content	6	0.0	1.0	4	4	0
Factor	Contribution	0	0.0	1.0	2	2	0
We	now	13	0.0	1.0	2	4	0
Convergence	Analysis	0	0.0	1.0	2	2	0
We	conduct	3	0.0	1.0	2	4	0
Relevance	Response	0	0.0	0.52	2	2	0
$$$	Mean	0	0.0	0.57	5	2	0
$$$	Average	0	0.0	0.57	5	2	0
$$$	Precision	0	0.0	0.57	5	2	0
$$$	0.4	0	0.0	0.52	5	4	0
$$$	0.45	0	0.0	0.52	5	4	0
$$$	0.5	0	0.0	0.52	5	4	0
$$$	0.55	0	0.0	0.52	5	4	0
$$$	0.6	0	0.0	0.52	5	4	0
$$$	0.65	0	0.0	0.52	5	4	0
$$$	0.7	0	0.0	0.52	5	4	0
$$$	0.75	0	0.0	0.52	5	4	0
$$$	0.8	0	0.0	0.52	5	4	0
0.85	RankFG	0	0.0	0.48	4	2	0
xxx	0	0	0.0	0	0	0	0
Figure	2:	1	0.0	1.0	0	3	0
#iterations	0	0	0.0	0.51	3	4	0
$$$	Mean	0	0.0	0.55	5	2	0
$$$	Average	0	0.0	0.55	5	2	0
$$$	Precision	0	0.0	0.55	5	2	0
$$$	0.5	0	0.0	0.5	5	4	0
$$$	0.55	0	0.0	0.5	5	4	0
$$$	0.6	0	0.0	0.5	5	4	0
$$$	0.65	0	0.0	0.5	5	4	0
$$$	0.7	0	0.0	0.5	5	4	0
$$$	0.75	0	0.0	0.5	5	4	0
$$$	0.8	0	0.0	0.5	5	4	0
$$$	0.85	0	0.0	0.5	5	4	0
Relevance	Response	0	0.0	0.45	2	2	0
Figure	3:	0	0.0	1.0	0	3	0
tions.	In	1	0.0	1.0	4	2	0
Training/Test	Ratio	0	0.0	1.0	2	2	0
We	provide	3	0.0	1.0	2	4	0
5	Related	0	0.0	1.2	1	2	0
The	research	4	0.0	1.0	2	4	0
Training	percentage	0	0.0	0.53	2	4	0
$$$	Mean	0	0.0	0.58	5	2	0
$$$	Average	0	0.0	0.58	5	2	0
$$$	Precision	0	0.0	0.58	5	2	0
$$$	0.5	0	0.0	0.52	5	4	0
$$$	0.55	0	0.0	0.52	5	4	0
$$$	0.6	0	0.0	0.52	5	4	0
$$$	0.65	0	0.0	0.52	5	4	0
$$$	0.7	0	0.0	0.52	5	4	0
$$$	0.75	0	0.0	0.52	5	4	0
$$$	0.8	0	0.0	0.52	5	4	0
Relevance	Response	0	0.0	0.47	2	2	0
Figure	4:	0	0.0	1.0	0	3	0
Mimno	and	10	0.0	1.0	2	4	0
6	Conclusions	0	0.0	1.2	1	2	0
In	this	9	0.0	1.0	2	4	0
$$$	References	0	0.0	1.2	5	6	0
[Benferhat	and	1	0.0	1.0	3	4	0
[Buckley	and	1	0.0	1.0	3	4	0
[Conry	et	1	0.0	1.0	3	4	0
[Craswell	et	1	0.0	1.0	3	4	0
[Dumais	and	1	0.0	1.0	3	4	0
[Hartvigsen	et	1	0.0	1.0	3	4	0
[Haym	et	1	0.0	1.0	3	4	0
[Hettich	and	1	0.0	1.0	3	4	0
[Karimzadehgan	and	1	0.0	1.0	3	4	0
[Karimzadehgan	et	1	0.0	1.0	3	4	0
[Kassirer	and	1	0.0	1.0	3	4	0
[Kindermann	et	1	0.0	1.0	3	4	0
[Kschischang	et	1	0.0	1.0	3	4	0
[Mauro	et	2	0.0	1.0	3	4	0
[Mimno	and	1	0.0	1.0	3	4	0
[Rennie,	1999]	1	0.0	1.0	3	3	0
[Smith,	1997]	1	0.0	1.0	3	3	0
[Sun	et	1	0.0	1.0	3	4	0
[Tang	et	1	0.0	1.0	3	4	0
[Tite	and	1	0.0	1.0	3	4	0
[Wu	et	1	0.0	1.0	3	4	0
[Yang	et	2	0.0	1.0	3	4	0
[Yedidia	et	1	0.0	1.0	3	4	0
[Zhai	and	1	0.0	1.0	3	4	0
[Zhang	et	1	0.0	1.0	3	4	0
